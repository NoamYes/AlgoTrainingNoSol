Read sections 10.1-10.5 of ESL (pages 337-346) about AdaBoost.

Explain to yourself the following, then discuss with your supervisor.

1. How does AdaBoost operate? What happens in every stage?

2. How are the weights updated in AdaBoost Simplify and explain qualitatively.

3. What is the loss function underlying AdaBoost? What relaxation is done in optimizing it?

4. Does it make sense to continue training AdaBoost after the accuracy has stabilized? Why?

5. What is the statistical quantity that the output of AdaBoost approximates? In what sense is the exponential loss equivalent to cross entropy? We will next discuss how they differ.

Read section 10.6.

7. Suppose you perform binary classification on a dataset where some of the labels are wrong. Which of cross-entropy and exponential loss is more suitable? Why?

Read about Gradient-Boosing Trees in 10.9-10.12, up to p.367.
